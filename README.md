# ðŸ§  Ollama Agent Demo â€“ Local LLM Playground

This repository contains a simple test setup to run and interact with LLMs locally using [Ollama](https://ollama.com) on a Mac M3 Pro (or any Apple Silicon).

## ðŸ”§ Requirements

- macOS with Apple Silicon (M1/M2/M3)
- Python 3.9+
- `requests` library (for API testing)
- Ollama installed (see below)

## ðŸš€ Setup

### 1. Install Ollama

```bash
curl -fsSL https://ollama.com/install.sh | sh
